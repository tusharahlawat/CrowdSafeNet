{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install torchsummary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-01-29T17:35:03.313993Z",
     "iopub.status.busy": "2025-01-29T17:35:03.313633Z",
     "iopub.status.idle": "2025-01-29T17:35:09.341882Z",
     "shell.execute_reply": "2025-01-29T17:35:09.340990Z",
     "shell.execute_reply.started": "2025-01-29T17:35:03.313964Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "import torchvision.datasets as datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torchsummary import summary\n",
    "import warnings\n",
    "import os\n",
    "import sys\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T17:35:09.343350Z",
     "iopub.status.busy": "2025-01-29T17:35:09.342917Z",
     "iopub.status.idle": "2025-01-29T17:35:09.415182Z",
     "shell.execute_reply": "2025-01-29T17:35:09.414375Z",
     "shell.execute_reply.started": "2025-01-29T17:35:09.343325Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T17:35:10.598374Z",
     "iopub.status.busy": "2025-01-29T17:35:10.598032Z",
     "iopub.status.idle": "2025-01-29T17:35:10.602925Z",
     "shell.execute_reply": "2025-01-29T17:35:10.602191Z",
     "shell.execute_reply.started": "2025-01-29T17:35:10.598336Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "print( torch.cuda.is_available() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T17:35:11.723008Z",
     "iopub.status.busy": "2025-01-29T17:35:11.722726Z",
     "iopub.status.idle": "2025-01-29T17:35:11.726665Z",
     "shell.execute_reply": "2025-01-29T17:35:11.725687Z",
     "shell.execute_reply.started": "2025-01-29T17:35:11.722986Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "RESOLUTION = 224"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T17:35:12.660741Z",
     "iopub.status.busy": "2025-01-29T17:35:12.660448Z",
     "iopub.status.idle": "2025-01-29T17:35:12.664613Z",
     "shell.execute_reply": "2025-01-29T17:35:12.663792Z",
     "shell.execute_reply.started": "2025-01-29T17:35:12.660719Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:1: SyntaxWarning: invalid escape sequence '\\.'\n",
      "<>:1: SyntaxWarning: invalid escape sequence '\\.'\n",
      "C:\\Users\\HP\\AppData\\Local\\Temp\\ipykernel_5268\\2051369251.py:1: SyntaxWarning: invalid escape sequence '\\.'\n",
      "  warnings.filterwarnings( \"ignore\", module = \"matplotlib\\..*\" )\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings( \"ignore\", module = \"matplotlib\\..*\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA AUGMENTATION AND VISUALIZING FUNCTIONS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T17:35:13.732217Z",
     "iopub.status.busy": "2025-01-29T17:35:13.731929Z",
     "iopub.status.idle": "2025-01-29T17:35:13.738156Z",
     "shell.execute_reply": "2025-01-29T17:35:13.737344Z",
     "shell.execute_reply.started": "2025-01-29T17:35:13.732196Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from torchvision.transforms import functional as F\n",
    "from tqdm import tqdm\n",
    "from typing import Callable, Any\n",
    "\n",
    "# Define the resolution\n",
    "RESOLUTION = 224\n",
    "\n",
    "# Custom data augmentation functions\n",
    "def random_horizontal_flip(image: Image.Image) -> Image.Image:\n",
    "    if torch.rand(1) < 0.5:\n",
    "        image = F.hflip(image)\n",
    "    return image\n",
    "\n",
    "def random_rotation(image: Image.Image) -> Image.Image:\n",
    "    angle = torch.randint(-10, 10, (1,))\n",
    "    image = F.rotate(image, angle.item())\n",
    "    return image\n",
    "\n",
    "# Define the transformation pipeline including multiple data augmentations\n",
    "transformer = transforms.Compose([\n",
    "    random_horizontal_flip,\n",
    "    random_rotation,\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5]),\n",
    "    transforms.Resize((RESOLUTION, RESOLUTION))\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T17:35:15.847810Z",
     "iopub.status.busy": "2025-01-29T17:35:15.847488Z",
     "iopub.status.idle": "2025-01-29T17:35:15.854659Z",
     "shell.execute_reply": "2025-01-29T17:35:15.853716Z",
     "shell.execute_reply.started": "2025-01-29T17:35:15.847783Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define a transform to resize images to 224x224\n",
    " \n",
    "\n",
    "def plot_image_from_list(images, labels, count):\n",
    "    plt.figure(figsize=(20, 20))  # Increase figure size for larger images\n",
    "    for i in range(count[0] * count[1]):\n",
    "        plt.subplot(count[0], count[1], i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        img = images[i].to(\"cpu\").numpy()\n",
    "        if img.shape[0] == 1:  # If the image is single-channel, assume it's grayscale\n",
    "            img = img.squeeze()\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(img.transpose((1, 2, 0)))  # Convert from CHW to HWC format\n",
    "        plt.xlabel(labels[i])\n",
    "    plt.show()\n",
    "\n",
    "def plot_randomly_from_dataset(dataset):\n",
    "    images = []  # List to store selected images\n",
    "    labels = []  # List to store corresponding labels\n",
    "\n",
    "    for _ in range(25):\n",
    "        index = torch.randint(0, len(dataset), (1,)).item()  # Randomly select an index\n",
    "        image, label = dataset[index]  # Retrieve image and label by index\n",
    "        image = resize_transform(image)  # Resize image to 224x224\n",
    "        images.append(image)\n",
    "        labels.append(dataset.label_id2str(label.item()))\n",
    "\n",
    "    plot_image_from_list(images, labels, (5, 5))  # Plot the images in a 5x5 grid\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**DATA PATHS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "DATA_SOURCE = {\n",
    "    \"Abuse\": \"/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Abuse\",\n",
    "    \"Arrest\": \"/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Arrest\",\n",
    "    \"Arson\": \"/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Arson\",\n",
    "    \"Assault\": \"/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-1/Assault\",\n",
    "    \"Burglary\": \"/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Burglary\",\n",
    "    \"Explosion\": \"/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Explosion\",\n",
    "    \"Fighting\": \"/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Anomaly-Videos-Part-2/Fighting\",\n",
    "    \"Normal\": \"/kaggle/input/crimeucfdataset/Anomaly_Dataset/Anomaly_Videos/Normal-Videos-Part-1\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TRAIN-TEST SPLIT AND VIDEO FRAMES GENERATION WITH DATA AUGMENTATION**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import os\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "\n",
    "# Assume transformer and DATA_SOURCE are defined somewhere else in the code\n",
    "\n",
    "class CrimeDataset(Dataset):\n",
    "    def __init__(self, train=True, train_test_split=0.8, random_state=42):\n",
    "        torch.manual_seed(random_state)  # Set the random seed for reproducibility\n",
    "        \n",
    "        self._data = []  # List to store the image data\n",
    "        self._labels = []  # List to store the corresponding labels\n",
    "        self._inclusion_probability = train_test_split if train else 1.0 - train_test_split  # Probability of including a file in the dataset\n",
    "        self._frame_interval = 30  # Interval to skip frames when reading video\n",
    "        \n",
    "        print(f\"Loading {'train' if train else 'test' } dataset...\")\n",
    "        for label, data_path in DATA_SOURCE.items():  # Iterate over the data sources\n",
    "            print(f\"Loading Label {label}...\")\n",
    "            for file in tqdm(os.listdir(data_path)):  # Iterate over the files in the data source directory\n",
    "                if file.endswith(\".mp4\") and torch.rand(1).item() <= self._inclusion_probability:\n",
    "                    path = os.path.join(data_path, file)\n",
    "                    data, labels = self._parse_file(path, label)  # Parse the video file\n",
    "                    self._data.extend(data)\n",
    "                    self._labels.extend(labels)\n",
    "        print(f\"Finished loading {'train' if train else 'test' } dataset... Loaded {len(self._data)} images.\")\n",
    "    \n",
    "    def _parse_file(self, path, label):\n",
    "        if not os.path.exists(path):\n",
    "            return [], []\n",
    "        \n",
    "        data = []  # List to store frames from the video\n",
    "        labels = []  # List to store labels corresponding to the frames\n",
    "        \n",
    "        cap = cv2.VideoCapture(path)  # Open the video file\n",
    "        \n",
    "        success, image = cap.read()\n",
    "        while success:\n",
    "            try:\n",
    "                if True:  # This block processes each frame\n",
    "                    image = cv2.cvtColor(image, cv2.COLOR_BGR2YUV)  # Convert frame to YUV color space\n",
    "                    Y, U, V = cv2.split(image)  # Split the frame into Y, U, and V components\n",
    "                    image = transformer(Image.fromarray(Y))  # Apply the transformation to the Y component\n",
    "                    data.append(image)  # Add the transformed frame to the data list\n",
    "                    labels.append(label)  # Add the label to the labels list\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading file {path}: {e}\")\n",
    "            \n",
    "            count = 0\n",
    "            while success and count < self._frame_interval:\n",
    "                success, image = cap.read()  # Read the next frame\n",
    "                count += 1\n",
    "        return data, labels\n",
    "        \n",
    "    def label_str2id(self, label):\n",
    "        labels = DATA_SOURCE.keys()\n",
    "        return list(labels).index(label)  # Convert string label to an ID\n",
    "\n",
    "    def label_id2str(self, label):\n",
    "        labels = DATA_SOURCE.keys()\n",
    "        return list(labels)[label]  # Convert label ID back to a string\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self._labels)  # Return the total number of samples in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        data = self._data[idx]  # Get the data (image) at the specified index\n",
    "        label = self._labels[idx]  # Get the label at the specified index\n",
    "        return data, torch.tensor([self.label_str2id(label)])  # Return the data and label as a tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create the training dataset\n",
    "train_dataset = CrimeDataset(True)  # Initialize with training data\n",
    "\n",
    "# Create the testing dataset\n",
    "test_dataset = CrimeDataset(False)  # Initialize with testing data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Load the test dataset randomly\n",
    "plot_randomly_from_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Create a DataLoader for the training dataset\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "\n",
    "# Create a DataLoader for the testing dataset\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T17:35:22.822127Z",
     "iopub.status.busy": "2025-01-29T17:35:22.821842Z",
     "iopub.status.idle": "2025-01-29T17:35:22.825575Z",
     "shell.execute_reply": "2025-01-29T17:35:22.824688Z",
     "shell.execute_reply.started": "2025-01-29T17:35:22.822106Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Define number of classes\n",
    "num_classes = 8"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Convulation Network (Feature Extraction)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T17:35:26.268021Z",
     "iopub.status.busy": "2025-01-29T17:35:26.267743Z",
     "iopub.status.idle": "2025-01-29T17:35:26.275919Z",
     "shell.execute_reply": "2025-01-29T17:35:26.275182Z",
     "shell.execute_reply.started": "2025-01-29T17:35:26.268002Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "class CrimeModelCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrimeModelCNN, self).__init__()\n",
    "        # First convolutional layer: input channels=1, output channels=64, kernel size=3x3, padding=same\n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=\"same\")\n",
    "        # Leaky ReLU activation function with a negative slope of 0.1\n",
    "        self.leaky_relu = nn.LeakyReLU(0.1)\n",
    "        # Max pooling layer with kernel size=2x2 and stride=2\n",
    "        self.max_pool1 = nn.MaxPool2d(2, 2)\n",
    "        # Dropout layer with a probability of 0.25\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        # Second convolutional layer: input channels=64, output channels=128, kernel size=3x3, padding=same\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=\"same\")\n",
    "        # Max pooling layer with kernel size=2x2 and stride=2\n",
    "        self.max_pool2 = nn.MaxPool2d(2, 2)\n",
    "        # Dropout layer with a probability of 0.25\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        # Third convolutional layer: input channels=128, output channels=256, kernel size=3x3, padding=same\n",
    "        self.conv3 = nn.Conv2d(128, 256, kernel_size=3, padding=\"same\")\n",
    "        # Max pooling layer with kernel size=2x2 and stride=2\n",
    "        self.max_pool3 = nn.MaxPool2d(2, 2)\n",
    "        # Dropout layer with a probability of 0.4\n",
    "        self.dropout3 = nn.Dropout(0.4)\n",
    "        # Flatten layer to convert the 3D tensor to a 1D tensor\n",
    "        self.flatten = nn.Flatten()\n",
    "        # Fully connected layer: input size=28*28*256, output size=256\n",
    "        self.fc1 = nn.Linear(28 * 28 * 256, 256)\n",
    "        # Dropout layer with a probability of 0.5\n",
    "        self.dropout4 = nn.Dropout(0.5)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x = self.conv1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.max_pool1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.max_pool2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.max_pool3(x)\n",
    "        x = self.dropout3(x)\n",
    "        x = self.flatten(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.leaky_relu(x)\n",
    "        x = self.dropout4(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**LSTM Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T17:35:28.505682Z",
     "iopub.status.busy": "2025-01-29T17:35:28.505347Z",
     "iopub.status.idle": "2025-01-29T17:35:28.510924Z",
     "shell.execute_reply": "2025-01-29T17:35:28.510016Z",
     "shell.execute_reply.started": "2025-01-29T17:35:28.505656Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class CrimeModelLSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrimeModelLSTM, self).__init__()\n",
    "        # Define the first LSTM layer: input_size=1, hidden_size=8, batch_first=True, bidirectional=False\n",
    "        self.lstm1 = nn.LSTM(1, 8, batch_first=True, bidirectional=False)\n",
    "        # Define the second LSTM layer: input_size=8, hidden_size=8, batch_first=True, bidirectional=False\n",
    "        self.lstm2 = nn.LSTM(8, 8, batch_first=True, bidirectional=False)\n",
    "        # Define the fully connected layer: input_size=8, output_size=4\n",
    "        self.fc = nn.Linear(8, 4)\n",
    "        # Define the drpout layer with a probability of 0.2\n",
    "        self.dropout = nn.Dropout(0.2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Forward pass through the network\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x = x[:, -1, :]  # Select the last output of the LSTM sequence\n",
    "        x = self.fc(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Conv + LSTM Network**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T17:35:30.544963Z",
     "iopub.status.busy": "2025-01-29T17:35:30.544680Z",
     "iopub.status.idle": "2025-01-29T17:35:30.550430Z",
     "shell.execute_reply": "2025-01-29T17:35:30.549171Z",
     "shell.execute_reply.started": "2025-01-29T17:35:30.544943Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class CrimeModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CrimeModel, self).__init__()\n",
    "        self.cnn = CrimeModelCNN()\n",
    "        self.lstm = CrimeModelLSTM()\n",
    "        self.fc = nn.Linear(260, 8)  # Adjust the input size according to your concatenation axis\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_cnn = x\n",
    "        x_lstm = torch.reshape(x, (x.shape[0], RESOLUTION * RESOLUTION, 1))\n",
    "        x_cnn = self.cnn(x_cnn)\n",
    "        x_lstm = self.lstm(x_lstm)\n",
    "        x_combined = torch.cat((x_cnn, x_lstm), dim=1)\n",
    "        x = self.fc(x_combined)\n",
    "#         return F.softmax(x, dim=1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T17:35:32.226924Z",
     "iopub.status.busy": "2025-01-29T17:35:32.226620Z",
     "iopub.status.idle": "2025-01-29T17:35:32.675870Z",
     "shell.execute_reply": "2025-01-29T17:35:32.674938Z",
     "shell.execute_reply.started": "2025-01-29T17:35:32.226900Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "model = CrimeModel()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL LAYERS**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataParallel(\n",
       "  (module): CrimeModel(\n",
       "    (cnn): CrimeModelCNN(\n",
       "      (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
       "      (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (dropout1): Dropout(p=0.25, inplace=False)\n",
       "      (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (dropout2): Dropout(p=0.25, inplace=False)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
       "      (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "      (dropout3): Dropout(p=0.4, inplace=False)\n",
       "      (flatten): Flatten(start_dim=1, end_dim=-1)\n",
       "      (fc1): Linear(in_features=200704, out_features=256, bias=True)\n",
       "      (dropout4): Dropout(p=0.5, inplace=False)\n",
       "    )\n",
       "    (lstm): CrimeModelLSTM(\n",
       "      (lstm1): LSTM(1, 8, batch_first=True)\n",
       "      (lstm2): LSTM(8, 8, batch_first=True)\n",
       "      (fc): Linear(in_features=8, out_features=4, bias=True)\n",
       "      (dropout): Dropout(p=0.2, inplace=False)\n",
       "    )\n",
       "    (fc): Linear(in_features=260, out_features=8, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = nn.DataParallel(model)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "NUM_EPOCHS = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for g in optimizer.param_groups:\n",
    "    g['lr'] = 0.001"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL TRAINING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "\n",
    "train_loss_history = []\n",
    "train_accuracy_history = []\n",
    "test_loss_history = []\n",
    "test_accuracy_history = []\n",
    "test_auc_roc_history = []\n",
    "\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    total_correct_train = 0\n",
    "    total_train_loss = 0\n",
    "\n",
    "    print(f\"Epoch : {epoch + 1}...\")\n",
    "\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        # Ensure label is a 1D tensor of class indices\n",
    "        label = label.squeeze()\n",
    "\n",
    "        preds = model(data)\n",
    "        loss = criterion(preds, label)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_train_loss += loss.item()\n",
    "        total_correct_train += (preds.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "        if batch_idx % 10 == 0:\n",
    "            print(f\"Loss: {loss.item()}, Batch: {batch_idx + 1}/{len(train_loader)}\", end=\"\\r\")\n",
    "\n",
    "    # Calculate average loss and accuracy for this epoch\n",
    "    avg_train_loss = total_train_loss / len(train_loader)\n",
    "    train_accuracy = total_correct_train / len(train_loader.dataset)\n",
    "\n",
    "    train_loss_history.append(avg_train_loss)\n",
    "    train_accuracy_history.append(train_accuracy)\n",
    "\n",
    "    print(f\"Train Loss: {avg_train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}\")\n",
    "\n",
    "    # Save the model after each epoch\n",
    "    torch.save(model.state_dict(), f\"crime_model_epoch_{epoch + 1}.pth\")\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    total_correct_test = 0\n",
    "    total_test_loss = 0\n",
    "    all_test_preds = []\n",
    "    all_test_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for data, label in test_loader:\n",
    "            data = data.to(device)\n",
    "            label = label.to(device)\n",
    "\n",
    "            # Ensure label is a 1D tensor of class indices\n",
    "            label = label.squeeze()\n",
    "\n",
    "            preds = model(data)\n",
    "            loss = criterion(preds, label)\n",
    "\n",
    "            total_test_loss += loss.item()\n",
    "            total_correct_test += (preds.argmax(dim=1) == label).sum().item()\n",
    "\n",
    "            # Normalize the predicted probabilities\n",
    "            preds_normalized = nn.functional.softmax(preds, dim=1)\n",
    "            all_test_preds.append(preds_normalized)\n",
    "            all_test_labels.append(label)\n",
    "\n",
    "    avg_test_loss = total_test_loss / len(test_loader)\n",
    "    test_accuracy = total_correct_test / len(test_loader.dataset)\n",
    "\n",
    "    test_loss_history.append(avg_test_loss)\n",
    "    test_accuracy_history.append(test_accuracy)\n",
    "    # Calculate and store AUC-ROC\n",
    "    all_test_preds = torch.cat(all_test_preds, dim=0)\n",
    "    all_test_labels = torch.cat(all_test_labels, dim=0)\n",
    "    test_auc_roc = roc_auc_score(all_test_labels.cpu(), all_test_preds.cpu(), multi_class='ovr')\n",
    "    test_auc_roc_history.append(test_auc_roc)\n",
    "\n",
    "    print(f\"Test Loss: {avg_test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}, Test AUC-ROC: {test_auc_roc:.4f}\")\n",
    "\n",
    "# Plotting the accuracies\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Training and Test Accuracy\")\n",
    "plt.plot(train_accuracy_history, label=\"Train Accuracy\")\n",
    "plt.plot(test_accuracy_history, label=\"Test Accuracy\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Training Loss\")\n",
    "plt.plot(train_loss_history, label=\"Train Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Test Loss\")\n",
    "plt.plot(test_loss_history, label=\"Test Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.title(\"Test AUC-ROC\")\n",
    "plt.plot(test_auc_roc_history, label=\"Test AUC-ROC\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"AUC-ROC\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_randomly_from_dataset_model(__dataset, __model):\n",
    "    images = []  # List to store selected images\n",
    "    labels = []  # List to store corresponding labels and model predictions\n",
    "\n",
    "    # Define the transformation to resize images to 224x224 and then convert to tensor\n",
    "    resize_transform = transforms.Resize((224, 224))\n",
    "    to_tensor_transform = transforms.ToTensor()\n",
    "\n",
    "    for _ in range(25):\n",
    "        index = torch.randint(0, len(__dataset), (1,)).item()  # Randomly select an index\n",
    "        image, label = __dataset[index]  # Retrieve image and label by index\n",
    "\n",
    "        # If the image is already a tensor, convert it back to PIL Image first\n",
    "        if isinstance(image, torch.Tensor):\n",
    "            image = transforms.ToPILImage()(image)\n",
    "\n",
    "        # Apply the resize transformation\n",
    "        image = resize_transform(image)\n",
    "        image_tensor = to_tensor_transform(image)  # Convert resized image to tensor\n",
    "        images.append(image_tensor)\n",
    "\n",
    "        # Get the model prediction for the image\n",
    "        with torch.no_grad():\n",
    "            image_tensor = image_tensor.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "            result = __model(image_tensor).squeeze().cpu().argmax()\n",
    "\n",
    "        # Append the actual label and predicted label to the labels list\n",
    "        labels.append(f\"{__dataset.label_id2str(label.item())} -> {__dataset.label_id2str(result.item())}\")\n",
    "\n",
    "    plot_image_from_list(images, labels, (5, 5))  # Plot the images in a 5x5 grid\n",
    "\n",
    "def plot_image_from_list(images, labels, count):\n",
    "    plt.figure(figsize=(20, 20))  # Increase figure size for larger images\n",
    "    for i in range(count[0] * count[1]):\n",
    "        plt.subplot(count[0], count[1], i + 1)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.grid(False)\n",
    "        img = images[i].to(\"cpu\").numpy()\n",
    "        if img.shape[0] == 1:  # If the image is single-channel, assume it's grayscale\n",
    "            img = img.squeeze()\n",
    "            plt.imshow(img, cmap=\"gray\")\n",
    "        else:\n",
    "            plt.imshow(img.transpose((1, 2, 0)))  # Convert from CHW to HWC format\n",
    "        plt.xlabel(labels[i])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**MODEL TESTING**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "model.to(\"cpu\")  # Move model to CPU before saving\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def predict_and_plot(image_path, model, label_id2str, device=None):\n",
    "    \"\"\"\n",
    "    Loads an image, applies transformations, makes a prediction using the model, \n",
    "    and plots the image with the predicted label.\n",
    "\n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        model (torch.nn.Module): Trained PyTorch model.\n",
    "        label_id2str (function): Function mapping label indices to class names.\n",
    "        device (str, optional): Device to run inference on ('cpu' or 'cuda'). \n",
    "                                Defaults to auto-detect.\n",
    "    \"\"\"\n",
    "    # Auto-detect device if not provided\n",
    "    if device is None:\n",
    "        device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    \n",
    "    # Define the transformation pipeline\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),  # Resize to match model input size\n",
    "        transforms.ToTensor(),          # Convert image to tensor\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Standard ImageNet normalization\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        # Load and preprocess the image\n",
    "        image = Image.open(image_path).convert(\"RGB\")  # Ensure it's RGB\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading image: {e}\")\n",
    "        return\n",
    "\n",
    "    # Apply transformations and add a batch dimension\n",
    "    image_tensor = transform(image).unsqueeze(0).to(device)  # Shape: [1, 3, 224, 224]\n",
    "\n",
    "    # Model inference\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(image_tensor).cpu().squeeze()\n",
    "        predicted_label = output.argmax().item()  # Get class index\n",
    "\n",
    "    # Convert label index to class name\n",
    "    predicted_label_str = label_id2str(predicted_label)\n",
    "\n",
    "    # Plot the image with prediction\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.grid(False)\n",
    "    plt.imshow(image)\n",
    "    plt.xlabel(f\"Prediction: {predicted_label_str}\", fontsize=14, color=\"blue\", fontweight='bold')\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T17:58:22.371783Z",
     "iopub.status.busy": "2025-01-29T17:58:22.371499Z",
     "iopub.status.idle": "2025-01-29T17:58:22.376217Z",
     "shell.execute_reply": "2025-01-29T17:58:22.375466Z",
     "shell.execute_reply.started": "2025-01-29T17:58:22.371762Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrimeModel(\n",
      "  (cnn): CrimeModelCNN(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1)\n",
      "    (max_pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout1): Dropout(p=0.25, inplace=False)\n",
      "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (max_pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout2): Dropout(p=0.25, inplace=False)\n",
      "    (conv3): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
      "    (max_pool3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "    (dropout3): Dropout(p=0.4, inplace=False)\n",
      "    (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "    (fc1): Linear(in_features=200704, out_features=256, bias=True)\n",
      "    (dropout4): Dropout(p=0.5, inplace=False)\n",
      "  )\n",
      "  (lstm): CrimeModelLSTM(\n",
      "    (lstm1): LSTM(1, 8, batch_first=True)\n",
      "    (lstm2): LSTM(8, 8, batch_first=True)\n",
      "    (fc): Linear(in_features=8, out_features=4, bias=True)\n",
      "    (dropout): Dropout(p=0.2, inplace=False)\n",
      "  )\n",
      "  (fc): Linear(in_features=260, out_features=8, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T18:07:14.981827Z",
     "iopub.status.busy": "2025-01-29T18:07:14.981482Z",
     "iopub.status.idle": "2025-01-29T18:07:15.174625Z",
     "shell.execute_reply": "2025-01-29T18:07:15.173795Z",
     "shell.execute_reply.started": "2025-01-29T18:07:14.981800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: Arrest\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(\"cuda\")\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Define transformations to match training preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale if needed\n",
    "    transforms.Resize((224, 224)),  # Resize to match the model's input shape\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize (adjust values as per training)\n",
    "])\n",
    "\n",
    "def test_single_image(image_path, model):\n",
    "    \"\"\"\n",
    "    Function to test the model on a single image.\n",
    "    \n",
    "    Args:\n",
    "        image_path (str): Path to the image file.\n",
    "        model (torch.nn.Module): Trained model.\n",
    "\n",
    "    Returns:\n",
    "        Predicted class label.\n",
    "    \"\"\"\n",
    "    # Load and preprocess image\n",
    "    image = Image.open(image_path).convert(\"L\")  # Convert to grayscale\n",
    "    image = transform(image)  # Apply transformations\n",
    "    image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        predicted_label = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    print(f\"Predicted Label: {label_id2str(predicted_label)}\")\n",
    "    return predicted_label\n",
    "\n",
    "# Function to test live feed from a camera\n",
    "def test_live_feed(frame, model):\n",
    "    \"\"\"\n",
    "    Function to test the model on a single frame from a live video feed.\n",
    "    \n",
    "    Args:\n",
    "        frame (np.array): Frame from video feed.\n",
    "        model (torch.nn.Module): Trained model.\n",
    "\n",
    "    Returns:\n",
    "        Predicted class label.\n",
    "    \"\"\"\n",
    "    image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert frame to grayscale\n",
    "    image = Image.fromarray(image)  # Convert NumPy array to PIL image\n",
    "    image = transform(image)  # Apply preprocessing\n",
    "    image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        output = model(image)\n",
    "        predicted_label = torch.argmax(output, dim=1).item()\n",
    "\n",
    "    return label_id2str(predicted_label)\n",
    "\n",
    "# Example usage\n",
    "test_single_image(\"gun.jpg\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Define transformations to match training preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale if needed\n",
    "    transforms.Resize((224, 224)),  # Resize to match the model's input shape\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize (adjust values as per training)\n",
    "])\n",
    "\n",
    "def process_video(video_path, model):\n",
    "    \"\"\"\n",
    "    Function to process a full video and make predictions on each frame.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        model (torch.nn.Module): Trained model.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)  # Open the video file\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "    \n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Exit loop if video ends\n",
    "        \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert frame to grayscale\n",
    "        image = Image.fromarray(image)  # Convert NumPy array to PIL image\n",
    "        image = transform(image)  # Apply preprocessing\n",
    "        image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(image)\n",
    "            predicted_label = torch.argmax(output, dim=1).item()\n",
    "        \n",
    "        # Display prediction on the frame\n",
    "        label_text = label_id2str(predicted_label)\n",
    "        cv2.putText(frame, f\"Prediction: {label_text}\", (10, 50), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Video Classification\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break  # Press 'q' to exit\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Example usage\n",
    "process_video(\"TEST2.mp4\", model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Arrest\n",
      "Metadata\n",
      "['Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest', 'Arrest']\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Ensure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# Define transformations to match training preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale if needed\n",
    "    transforms.Resize((224, 224)),  # Resize to match the model's input shape\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize (adjust values as per training)\n",
    "])\n",
    "\n",
    "def label_id2str(label_id):\n",
    "    label_map = {0: \"Abuse\", 1: \"Arrest\", 2: \"Arson\", 3: \"Assault\", 4: \"Burglary\", 5: \"Explosion\", 6: \"Fighting\", 7: \"Normal\"}\n",
    "    return label_map.get(label_id, \"Unknown\")\n",
    "\n",
    "def process_video(video_path, model):\n",
    "    \"\"\"\n",
    "    Function to process a full video and make predictions on each frame.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        model (torch.nn.Module): Trained model.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)  # Open the video file\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "    \n",
    "    metadata = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Exit loop if video ends\n",
    "        \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert frame to grayscale\n",
    "        image = Image.fromarray(image)  # Convert NumPy array to PIL image\n",
    "        image = transform(image)  # Apply preprocessing\n",
    "        image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(image)\n",
    "            predicted_label = torch.argmax(output, dim=1).item()\n",
    "        \n",
    "        # Get the class name and print it to the backend\n",
    "        label_text = label_id2str(predicted_label)\n",
    "        print(f\"{label_text}\")  # Printing to the backend/log\n",
    "\n",
    "        # Store label in metadata list\n",
    "        metadata.append(label_text)\n",
    "        \n",
    "        # Display prediction on the frame\n",
    "        cv2.putText(frame, f\"Prediction: {label_text}\", (10, 50), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show the frame\n",
    "        cv2.imshow(\"Video Classification\", frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break  # Press 'q' to exit\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Return metadata for potential use in frontend\n",
    "    return metadata\n",
    "\n",
    "# Example usage\n",
    "metadata = process_video(\"TEST2.mp4\", model)\n",
    "\n",
    "# Now metadata contains the list of predicted labels per frame.\n",
    "print(\"Metadata\")\n",
    "print(metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[36], line 101\u001b[0m\n\u001b[0;32m     98\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m class_list, threat_level_list\n\u001b[0;32m    100\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m--> 101\u001b[0m class_list, threat_level_list \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfinal.mp4\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Now class_list contains the predicted classes, and threat_level_list contains the corresponding threat levels.\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mClass List:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[36], line 50\u001b[0m, in \u001b[0;36mprocess_video\u001b[1;34m(video_path, model)\u001b[0m\n\u001b[0;32m     48\u001b[0m image \u001b[38;5;241m=\u001b[39m cv2\u001b[38;5;241m.\u001b[39mcvtColor(frame, cv2\u001b[38;5;241m.\u001b[39mCOLOR_BGR2GRAY)  \u001b[38;5;66;03m# Convert frame to grayscale\u001b[39;00m\n\u001b[0;32m     49\u001b[0m image \u001b[38;5;241m=\u001b[39m Image\u001b[38;5;241m.\u001b[39mfromarray(image)  \u001b[38;5;66;03m# Convert NumPy array to PIL image\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m image \u001b[38;5;241m=\u001b[39m \u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Apply preprocessing\u001b[39;00m\n\u001b[0;32m     51\u001b[0m image \u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)  \u001b[38;5;66;03m# Add batch dimension and move to device\u001b[39;00m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Make prediction\u001b[39;00m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:95\u001b[0m, in \u001b[0;36mCompose.__call__\u001b[1;34m(self, img)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, img):\n\u001b[0;32m     94\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransforms:\n\u001b[1;32m---> 95\u001b[0m         img \u001b[38;5;241m=\u001b[39m \u001b[43mt\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     96\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m img\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1737\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1738\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1739\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1745\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1746\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1747\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1748\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1749\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1750\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1752\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1753\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\torchvision\\transforms\\transforms.py:277\u001b[0m, in \u001b[0;36mNormalize.forward\u001b[1;34m(self, tensor)\u001b[0m\n\u001b[0;32m    269\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, tensor: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    270\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m    272\u001b[0m \u001b[38;5;124;03m        tensor (Tensor): Tensor image to be normalized.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    275\u001b[0m \u001b[38;5;124;03m        Tensor: Normalized Tensor image.\u001b[39;00m\n\u001b[0;32m    276\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 277\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\torchvision\\transforms\\functional.py:350\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    347\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(tensor, torch\u001b[38;5;241m.\u001b[39mTensor):\n\u001b[0;32m    348\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimg should be Tensor Image. Got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(tensor)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 350\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF_t\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnormalize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmean\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstd\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Python\\Lib\\site-packages\\torchvision\\transforms\\_functional_tensor.py:928\u001b[0m, in \u001b[0;36mnormalize\u001b[1;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[0;32m    926\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m std\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    927\u001b[0m     std \u001b[38;5;241m=\u001b[39m std\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m--> 928\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msub_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmean\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdiv_(std)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "packet queue is empty, aborting\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import base64\n",
    "import socketio\n",
    "import time\n",
    "# Ensure the model is on the correct device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "model.eval()  # Set model to evaluation mode\n",
    "sio = socketio.Client()\n",
    "sio.connect('http://127.0.0.1:5000')\n",
    "\n",
    "# Define transformations to match training preprocessing\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),  # Convert to grayscale if needed\n",
    "    transforms.Resize((224, 224)),  # Resize to match the model's input shape\n",
    "    transforms.ToTensor(),  # Convert image to PyTorch tensor\n",
    "    transforms.Normalize(mean=[0.5], std=[0.5])  # Normalize (adjust values as per training)\n",
    "])\n",
    "\n",
    "\n",
    "def process_video(video_path, model):\n",
    "    \"\"\"\n",
    "    Function to process a full video and make predictions on each frame.\n",
    "    \n",
    "    Args:\n",
    "        video_path (str): Path to the video file.\n",
    "        model (torch.nn.Module): Trained model.\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)  # Open the video file\n",
    "    \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "    \n",
    "    class_list = []  # To store predicted classes\n",
    "    threat_level_list = []  # To store corresponding threat levels\n",
    "    frame_interval = 1.0 / 10\n",
    "    last_time = time.time()\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break  # Exit loop if video ends\n",
    "        \n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert frame to grayscale\n",
    "        image = Image.fromarray(image)  # Convert NumPy array to PIL image\n",
    "        image = transform(image)  # Apply preprocessing\n",
    "        image = image.unsqueeze(0).to(device)  # Add batch dimension and move to device\n",
    "\n",
    "        # Make prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(image)\n",
    "            predicted_label = torch.argmax(output, dim=1).item()\n",
    "        \n",
    "        # Get the class name\n",
    "        label_text = label_id2str(predicted_label)\n",
    "        \n",
    "        # Get the corresponding threat level\n",
    "        threat_level = get_threat_level(label_text)\n",
    "        \n",
    "        # Print the class and threat level to the backend\n",
    "        alert = f\" {label_text}\" \n",
    "        threat = f\" {threat_level}\"\n",
    "        sio.emit('Alert', {'data': alert})\n",
    "        sio.emit('Threat', {'data': threat})\n",
    "        \n",
    "        # Store class and threat level in separate lists\n",
    "        class_list.append(label_text)\n",
    "        threat_level_list.append(threat_level)\n",
    "        \n",
    "        # Display prediction and threat level on the frame\n",
    "        cv2.putText(frame, f\"Prediction: {label_text}\", (10, 50), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "        cv2.putText(frame, f\"Threat level: {threat_level}\", (10, 100), \n",
    "                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "        \n",
    "        # Show the frame\n",
    "        \n",
    "        frame = cv2.resize(frame, (640, 480))  # Resize the frame\n",
    "\n",
    "        _, buffer = cv2.imencode('.jpg', frame)  # Convert frame to JPG format\n",
    "        frame_bytes = buffer.tobytes()  # Convert the buffer to a byte object\n",
    "\n",
    "        # Encode the byte object to base64\n",
    "        frame_base64 = base64.b64encode(frame_bytes).decode('utf-8')  # Base64 encode the frame\n",
    "\n",
    "        # Emit the base64 frame to the socket server\n",
    "        sio.emit('video_frame_1', {'data': frame_base64})\n",
    "        \n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "    # Return both lists for potential use in frontend or further processing\n",
    "    return class_list, threat_level_list\n",
    "\n",
    "# Example usage\n",
    "class_list, threat_level_list = process_video(\"final.mp4\", model)\n",
    "\n",
    "# Now class_list contains the predicted classes, and threat_level_list contains the corresponding threat levels.\n",
    "print(\"Class List:\")\n",
    "print(class_list)\n",
    "print(\"\\nThreat Level List:\")\n",
    "print(threat_level_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T18:05:33.040927Z",
     "iopub.status.busy": "2025-01-29T18:05:33.040651Z",
     "iopub.status.idle": "2025-01-29T18:05:33.044922Z",
     "shell.execute_reply": "2025-01-29T18:05:33.044092Z",
     "shell.execute_reply.started": "2025-01-29T18:05:33.040906Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def label_id2str(label_id):\n",
    "    label_map = {0: \"Abuse\", 1: \"Arrest\", 2: \"Arson\", 3: \"Assault\", 4: \"Burglary\", 5: \"Explosion\", 6: \"Fighting\", 7: \"Normal\"}\n",
    "    return label_map.get(label_id, \"Unknown\")\n",
    "\n",
    "def get_threat_level(label):\n",
    "    threat_levels = {\n",
    "        \"Abuse\": \"Low\",\n",
    "        \"Arrest\": \"Low\",\n",
    "        \"Arson\": \"High\",\n",
    "        \"Assault\": \"Medium\",\n",
    "        \"Burglary\": \"Medium\",\n",
    "        \"Explosion\": \"High\",\n",
    "        \"Fighting\": \"Medium\",\n",
    "        \"Normal\": \"No threat\"\n",
    "    }\n",
    "    return threat_levels.get(label, \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T17:57:47.512011Z",
     "iopub.status.busy": "2025-01-29T17:57:47.511731Z",
     "iopub.status.idle": "2025-01-29T17:57:47.940459Z",
     "shell.execute_reply": "2025-01-29T17:57:47.939772Z",
     "shell.execute_reply.started": "2025-01-29T17:57:47.511991Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "model=CrimeModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-29T18:05:28.807446Z",
     "iopub.status.busy": "2025-01-29T18:05:28.807127Z",
     "iopub.status.idle": "2025-01-29T18:05:29.038017Z",
     "shell.execute_reply": "2025-01-29T18:05:29.037376Z",
     "shell.execute_reply.started": "2025-01-29T18:05:28.807422Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded from Crime_model2 (1).pth\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def save_model(model, path=\"crime_model.pth\"):\n",
    "    \"\"\"\n",
    "    Saves the trained model while ensuring it is compatible for reloading.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): Trained model.\n",
    "        path (str): Path to save the model.\n",
    "    \"\"\"\n",
    "    if isinstance(model, torch.nn.DataParallel):\n",
    "        model_state = model.module.state_dict()  # Remove 'module.' prefix\n",
    "    else:\n",
    "        model_state = model.state_dict()\n",
    "    \n",
    "    torch.save(model_state, path)\n",
    "    print(f\"Model saved to {path}\")\n",
    "\n",
    "def load_model(model, path=\"crime_model.pth\"):\n",
    "    \"\"\"\n",
    "    Loads the trained model from a file and ensures compatibility.\n",
    "    \n",
    "    Args:\n",
    "        model (torch.nn.Module): Model architecture (must be defined before calling this).\n",
    "        path (str): Path to the saved model.\n",
    "\n",
    "    Returns:\n",
    "        torch.nn.Module: Model with loaded weights.\n",
    "    \"\"\"\n",
    "    state_dict = torch.load(path, map_location=torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    # Fix issue with \"module.\" prefix in keys when loading DataParallel-trained models\n",
    "    new_state_dict = {k.replace(\"module.\", \"\"): v for k, v in state_dict.items()}\n",
    "\n",
    "    model.load_state_dict(new_state_dict)\n",
    "    model.to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "    model.eval()\n",
    "    print(f\"Model loaded from {path}\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "# Example usage:\n",
    "#save_model(model, \"crime_model_cpu.pth\")\n",
    "model = load_model(model, \"Crime_model2 (1).pth\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 75548,
     "sourceId": 170620,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 2491957,
     "sourceId": 7408193,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6567606,
     "sourceId": 10609007,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6567971,
     "sourceId": 10609484,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6568345,
     "sourceId": 10609987,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6569661,
     "sourceId": 10611860,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6569670,
     "sourceId": 10611871,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6569686,
     "sourceId": 10611894,
     "sourceType": "datasetVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 230758,
     "modelInstanceId": 209066,
     "sourceId": 244721,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 231043,
     "modelInstanceId": 209356,
     "sourceId": 245050,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 231061,
     "modelInstanceId": 209377,
     "sourceId": 245071,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 231115,
     "modelInstanceId": 209430,
     "sourceId": 245130,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
